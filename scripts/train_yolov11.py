"""
Training script for YOLOv11 models (n, s, m variants)
For Coffee Bean Ripeness Detection

Usage:
    # Train YOLOv11n (Nano - fastest, smallest)
    python scripts/train_yolov11.py --model yolov11n --epochs 100

    # Train YOLOv11s (Small - balanced)
    python scripts/train_yolov11.py --model yolov11s --epochs 100

    # Train YOLOv11m (Medium - best accuracy)
    python scripts/train_yolov11.py --model yolov11m --epochs 100

    # With custom settings
    python scripts/train_yolov11.py --model yolov11s --epochs 150 --batch 16 --imgsz 640

Model comparison:
    | Model    | Params | mAP@0.5 (COCO) | Speed  | Recommended for          |
    |----------|--------|----------------|--------|--------------------------|
    | YOLOv11n | 2.6M   | 39.5%          | 1.5ms  | Mobile, edge devices     |
    | YOLOv11s | 9.4M   | 47.0%          | 2.5ms  | Balanced speed/accuracy  |
    | YOLOv11m | 20.1M  | 51.5%          | 4.7ms  | Best accuracy            |
"""

import argparse
import os
import sys
from pathlib import Path
from datetime import datetime

# Add project root to path
PROJECT_ROOT = Path(__file__).parent.parent
sys.path.insert(0, str(PROJECT_ROOT))


def check_ultralytics():
    """Check if ultralytics is installed and has YOLOv11 support"""
    try:
        from ultralytics import YOLO
        import ultralytics
        print(f"Ultralytics version: {ultralytics.__version__}")
        return True
    except ImportError:
        print("ERROR: ultralytics not installed!")
        print("Install with: pip install ultralytics>=8.3.0")
        return False


def create_dataset_yaml(data_dir: Path, output_path: Path):
    """
    Create dataset.yaml file for YOLO training

    Expected directory structure:
    data_dir/
    ├── train/
    │   ├── images/
    │   └── labels/
    ├── val/
    │   ├── images/
    │   └── labels/
    └── test/ (optional)
        ├── images/
        └── labels/
    """

    # Check directories exist
    train_images = data_dir / "train" / "images"
    val_images = data_dir / "val" / "images"

    if not train_images.exists():
        print(f"WARNING: Train images directory not found: {train_images}")
    if not val_images.exists():
        print(f"WARNING: Val images directory not found: {val_images}")

    # Coffee bean classes
    classes = [
        "barely-riped",
        "over-riped",
        "riped",
        "semi-riped",
        "unriped"
    ]

    yaml_content = f"""# Coffee Bean Ripeness Detection Dataset
# Auto-generated by train_yolov11.py

path: {data_dir.absolute()}
train: train/images
val: val/images
test: test/images  # optional

# Classes
names:
  0: barely-riped
  1: over-riped
  2: riped
  3: semi-riped
  4: unriped

# Number of classes
nc: 5
"""

    with open(output_path, 'w') as f:
        f.write(yaml_content)

    print(f"Created dataset config: {output_path}")
    return output_path


def train_yolov11(
    model_variant: str = "yolov11n",
    data_yaml: str = None,
    epochs: int = 100,
    batch_size: int = 16,
    img_size: int = 640,
    device: str = "auto",
    workers: int = 8,
    patience: int = 50,
    save_period: int = 10,
    resume: bool = False,
    pretrained: bool = True,
    optimizer: str = "auto",
    lr0: float = 0.01,
    lrf: float = 0.01,
    augment: bool = True,
    project: str = None,
    name: str = None,
    exist_ok: bool = False,
    verbose: bool = True
):
    """
    Train YOLOv11 model

    Args:
        model_variant: Model size - 'yolov11n', 'yolov11s', 'yolov11m', 'yolov11l', 'yolov11x'
        data_yaml: Path to dataset.yaml file
        epochs: Number of training epochs
        batch_size: Batch size (-1 for auto)
        img_size: Input image size
        device: Training device ('cpu', '0', '0,1', 'auto')
        workers: Number of dataloader workers
        patience: Early stopping patience (epochs)
        save_period: Save checkpoint every N epochs
        resume: Resume from last checkpoint
        pretrained: Use pretrained weights
        optimizer: Optimizer ('SGD', 'Adam', 'AdamW', 'auto')
        lr0: Initial learning rate
        lrf: Final learning rate factor
        augment: Enable augmentation
        project: Project directory for saving results
        name: Experiment name
        exist_ok: Overwrite existing experiment
        verbose: Verbose output

    Returns:
        Training results
    """

    from ultralytics import YOLO

    # Validate model variant
    valid_models = ['yolov11n', 'yolov11s', 'yolov11m', 'yolov11l', 'yolov11x']
    if model_variant not in valid_models:
        raise ValueError(f"Invalid model: {model_variant}. Choose from {valid_models}")

    # Set default paths
    if data_yaml is None:
        data_yaml = PROJECT_ROOT / "dataset.yaml"

    if project is None:
        project = PROJECT_ROOT / "runs" / "train"

    if name is None:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        name = f"{model_variant}_coffee_{timestamp}"

    # Model weights
    if pretrained:
        weights = f"{model_variant}.pt"  # Will download from Ultralytics
    else:
        weights = f"{model_variant}.yaml"  # Train from scratch

    print("=" * 60)
    print(f"Training {model_variant.upper()} for Coffee Bean Detection")
    print("=" * 60)
    print(f"Model: {weights}")
    print(f"Dataset: {data_yaml}")
    print(f"Epochs: {epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Image size: {img_size}")
    print(f"Device: {device}")
    print(f"Project: {project}")
    print(f"Name: {name}")
    print("=" * 60)

    # Load model
    model = YOLO(weights)

    # Training arguments
    train_args = {
        'data': str(data_yaml),
        'epochs': epochs,
        'batch': batch_size,
        'imgsz': img_size,
        'device': device if device != 'auto' else None,
        'workers': workers,
        'patience': patience,
        'save_period': save_period,
        'resume': resume,
        'optimizer': optimizer,
        'lr0': lr0,
        'lrf': lrf,
        'project': str(project),
        'name': name,
        'exist_ok': exist_ok,
        'verbose': verbose,

        # Augmentation settings for coffee beans
        'hsv_h': 0.015,      # HSV-Hue augmentation
        'hsv_s': 0.7,        # HSV-Saturation augmentation
        'hsv_v': 0.4,        # HSV-Value augmentation
        'degrees': 0.0,      # Rotation (disabled for coffee)
        'translate': 0.1,    # Translation
        'scale': 0.5,        # Scale
        'shear': 0.0,        # Shear (disabled)
        'perspective': 0.0,  # Perspective (disabled)
        'flipud': 0.0,       # Vertical flip (disabled - beans have orientation)
        'fliplr': 0.5,       # Horizontal flip
        'mosaic': 1.0,       # Mosaic augmentation
        'mixup': 0.0,        # Mixup (disabled)
        'copy_paste': 0.0,   # Copy-paste (disabled)
    }

    if not augment:
        # Disable augmentations
        for key in ['hsv_h', 'hsv_s', 'hsv_v', 'degrees', 'translate',
                    'scale', 'shear', 'perspective', 'flipud', 'fliplr',
                    'mosaic', 'mixup', 'copy_paste']:
            train_args[key] = 0.0

    # Start training
    results = model.train(**train_args)

    print("\n" + "=" * 60)
    print("Training Complete!")
    print("=" * 60)
    print(f"Best weights: {project}/{name}/weights/best.pt")
    print(f"Last weights: {project}/{name}/weights/last.pt")
    print(f"Results: {project}/{name}/")

    return results


def validate_model(weights_path: str, data_yaml: str = None, img_size: int = 640):
    """Validate trained model"""
    from ultralytics import YOLO

    if data_yaml is None:
        data_yaml = PROJECT_ROOT / "dataset.yaml"

    model = YOLO(weights_path)
    results = model.val(data=str(data_yaml), imgsz=img_size)

    return results


def export_model(weights_path: str, format: str = "onnx"):
    """
    Export model to different formats

    Formats: 'torchscript', 'onnx', 'openvino', 'engine', 'coreml', 'tflite'
    """
    from ultralytics import YOLO

    model = YOLO(weights_path)
    model.export(format=format)

    print(f"Exported model to {format} format")


def main():
    parser = argparse.ArgumentParser(
        description="Train YOLOv11 for Coffee Bean Detection",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
    # Train YOLOv11n (fastest)
    python scripts/train_yolov11.py --model yolov11n --epochs 100

    # Train YOLOv11s (balanced)
    python scripts/train_yolov11.py --model yolov11s --epochs 150 --batch 16

    # Train YOLOv11m (best accuracy)
    python scripts/train_yolov11.py --model yolov11m --epochs 200 --batch 8

    # Resume training
    python scripts/train_yolov11.py --model yolov11s --resume

    # Validate model
    python scripts/train_yolov11.py --validate --weights runs/train/yolov11s_coffee/weights/best.pt

    # Export model
    python scripts/train_yolov11.py --export --weights best.pt --format onnx
        """
    )

    # Model selection
    parser.add_argument('--model', type=str, default='yolov11n',
                        choices=['yolov11n', 'yolov11s', 'yolov11m', 'yolov11l', 'yolov11x'],
                        help='Model variant (default: yolov11n)')

    # Dataset
    parser.add_argument('--data', type=str, default=None,
                        help='Path to dataset.yaml (default: ./dataset.yaml)')
    parser.add_argument('--data-dir', type=str, default=None,
                        help='Path to dataset directory (auto-create dataset.yaml)')

    # Training parameters
    parser.add_argument('--epochs', type=int, default=100,
                        help='Number of epochs (default: 100)')
    parser.add_argument('--batch', type=int, default=16,
                        help='Batch size (default: 16, -1 for auto)')
    parser.add_argument('--imgsz', type=int, default=640,
                        help='Image size (default: 640)')
    parser.add_argument('--device', type=str, default='auto',
                        help='Device: cpu, 0, 0,1, mps, auto (default: auto)')
    parser.add_argument('--workers', type=int, default=8,
                        help='Dataloader workers (default: 8)')

    # Training options
    parser.add_argument('--patience', type=int, default=50,
                        help='Early stopping patience (default: 50)')
    parser.add_argument('--resume', action='store_true',
                        help='Resume from last checkpoint')
    parser.add_argument('--no-pretrained', action='store_true',
                        help='Train from scratch (no pretrained weights)')
    parser.add_argument('--no-augment', action='store_true',
                        help='Disable augmentation')

    # Optimizer
    parser.add_argument('--optimizer', type=str, default='auto',
                        choices=['SGD', 'Adam', 'AdamW', 'auto'],
                        help='Optimizer (default: auto)')
    parser.add_argument('--lr0', type=float, default=0.01,
                        help='Initial learning rate (default: 0.01)')

    # Output
    parser.add_argument('--project', type=str, default=None,
                        help='Project directory (default: runs/train)')
    parser.add_argument('--name', type=str, default=None,
                        help='Experiment name (default: auto)')
    parser.add_argument('--exist-ok', action='store_true',
                        help='Overwrite existing experiment')

    # Actions
    parser.add_argument('--validate', action='store_true',
                        help='Run validation only')
    parser.add_argument('--export', action='store_true',
                        help='Export model')
    parser.add_argument('--weights', type=str, default=None,
                        help='Weights path for validation/export')
    parser.add_argument('--format', type=str, default='onnx',
                        help='Export format (default: onnx)')

    args = parser.parse_args()

    # Check ultralytics
    if not check_ultralytics():
        sys.exit(1)

    # Handle actions
    if args.validate:
        if args.weights is None:
            print("ERROR: --weights required for validation")
            sys.exit(1)
        validate_model(args.weights, args.data, args.imgsz)
        return

    if args.export:
        if args.weights is None:
            print("ERROR: --weights required for export")
            sys.exit(1)
        export_model(args.weights, args.format)
        return

    # Create dataset.yaml if needed
    data_yaml = args.data
    if data_yaml is None and args.data_dir:
        data_dir = Path(args.data_dir)
        data_yaml = PROJECT_ROOT / "dataset.yaml"
        create_dataset_yaml(data_dir, data_yaml)
    elif data_yaml is None:
        data_yaml = PROJECT_ROOT / "dataset.yaml"
        if not Path(data_yaml).exists():
            print(f"ERROR: Dataset config not found: {data_yaml}")
            print("Create dataset.yaml or use --data-dir to auto-generate")
            sys.exit(1)

    # Train
    train_yolov11(
        model_variant=args.model,
        data_yaml=data_yaml,
        epochs=args.epochs,
        batch_size=args.batch,
        img_size=args.imgsz,
        device=args.device,
        workers=args.workers,
        patience=args.patience,
        resume=args.resume,
        pretrained=not args.no_pretrained,
        optimizer=args.optimizer,
        lr0=args.lr0,
        augment=not args.no_augment,
        project=args.project,
        name=args.name,
        exist_ok=args.exist_ok
    )


if __name__ == "__main__":
    main()
